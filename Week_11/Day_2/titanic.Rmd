---
title: "R Notebook"
output: html_notebook
---

# Decision trees homework

In this homework we will create a decision tree to see which factors are useful in predicting whether or not a passenger on the titanic will survive.

Run the code below before you begin:

```{r}
library(rpart)
library(rpart.plot)
library(tidyverse)
library(GGally)
library(modelr)
library(yardstick)
library(caret)
library(ranger)
```


```{r}
titanic_set <- read_csv('data/titanic_decision_tree_data.csv')

shuffle_index <- sample(1:nrow(titanic_set))

# shuffle the data so class order isn't in order - need this for training/testing split later on 
titanic_set <- titanic_set[shuffle_index, ]
```



# Data Dictionary

sex: Biological Sex, male or female
age_status: adult or child (child defined as under 16)
class : Ticket class, 1 = 1st (Upper class), 2 = 2nd (Middle Class), 3 = 3rd (Lower Class)
port_embarkation: C = Cherbourg, Q = Queenstown, S = Southampton
sibsp : number of siblings / spouses aboard the Titanic
parch: number of parents / children aboard the Titanic. Some children travelled only with a nanny, therefore parch=0 for them.
survived_flag : did they survive, 0 = No, 1 = Yes

# 1 MVP
## 1.1 Question 1

Cleaning up the data is always the first step. Do the following:

Take only observations which have a survived flag (i.e. that aren’t missing)
Turn your important variables into factors (sex, survived, pclass, embarkation)
Create an age_status variable which groups individuals under (and including) 16 years of age into a category called “child” category and those over 16 into a category called “adult”.
Drop the NA
Drop any variables you don’t need (X1, passenger_id, name, ticket, far, cabin)
If you need help doing this, the code is below, but please try it yourself first so you can learn!


### Data Cleaning Code

```{r}
titanic_clean <- titanic_set %>%
  filter(survived %in% c(0,1)) %>%
# Convert to factor level
    mutate(sex = as.factor(sex), 
           age_status = as.factor(if_else(age <= 16, "child", "adult")),
           class = factor(pclass, levels = c(3,2,1), labels = 
                            c("Lower", "Middle", "Upper")), 
           survived_flag = factor(survived, levels = c(0,1), labels = 
                                    c("No", "Yes")), 
           port_embarkation = as.factor(embarked)) %>%
  select(sex, age_status, class, port_embarkation, sib_sp, parch, survived_flag) %>%
  na.omit()
```

## 1.2 Question 2

Have a look at your data and create some plots to ensure you know what you’re working with before you begin. Write a summary of what you have found in your plots. Which variables do you think might be useful to predict whether or not people are going to die? Knowing this before you start is the best way to have a sanity check that your model is doing a good job.

```{r}
summary(titanic_clean)
```
```{r}
titanic_clean %>% 
  ggpairs()
```
Using ggpairs() I ran a simple comparisons between the datasets. The sets I think show significant relationships with the survived flag are parch, class and sex - so I will start with those variables.

## 1.3 Question 3

Now you can start to build your model. Create your testing and training set using an appropriate split. Check you have balanced sets. Write down why you chose the split you did and produce output tables to show whether or not it is balanced. [Extra - if you want to force balanced testing and training sets, have a look at the stratified() function in package splitstackshape (you can specify multiple variables to stratify on by passing a vector of variable names to the group argument, and get back testing and training sets with argument bothSets = TRUE)]

```{r}
# Partition the data
# get how many rows we have in total to work out the percentage
n_data <- nrow(titanic_clean)

# create a test sample index
test_index <- sample(1:n_data, size = n_data*0.2)

# create test set
titanic_test  <- slice(titanic_clean, test_index)

# create training set
titanic_train <- slice(titanic_clean, -test_index)
```

To prevent over-fitting it is important to split the date *before* creating the model. So that you can then use the test data to test the effectiveness of the model.

The training data set needs to contain the bulk of the data so an 80/20 split is a good balance.

```{r}
# Check for balance in the datasets
titanic_test %>%
 janitor::tabyl(survived_flag)

titanic_train %>%
 janitor::tabyl(survived_flag)
```

The test data set has a split of 61:39 (%)
The train data set has a split of 59:41 (%)
They do not vary hugely so we can proceed.


## 1.4 Question 4

Create your decision tree to try and predict survival probability using an appropriate method, and create a decision tree plot.

```{r}
titanic_fit <- rpart(
  formula = survived_flag ~ ., 
  data = titanic_train, 
  method = 'class'
)

rpart.plot(titanic_fit, 
           yesno = 2, 
           fallen.leaves = TRUE, 
           faclen = 6, 
           digits = 4)
```


```{r}
rpart.plot(titanic_fit, 
           yesno = 2, 
           fallen.leaves = TRUE, 
           faclen = 6, 
           digits = 4, 
           type = 4, 
           extra = 101)
```

## 1.5 Question 5

Write down what this tells you, in detail. What variables are important? What does each node tell you? Who has the highest chance of surviving? Who has the lowest? Provide as much detail as you can. 

To start with, the variables it has picked are the ones that are deemed most informative for predicting whether someone will survive (or not). The rest have been discarded from our model. In our case it has picked class and port embarkation

Each node contains three pieces of information:
  The predicted result for a data point at the node (Survived or Died in this example) is on the top line. For the root node, this means that if you look at all the data together, the most likely result is that they did not survive.
  The second line contains the probability of surviving as a decimal. So, overall this is 40.88%.
  The third line is the percentage of data points which pass through this node.
  
The colouring helps us see at a glance where the survivors are - thet are in green and those that died are in blue. The intensity of the colour increases for the larger numbers.

The second decision tree shows us the numbers in each set and how they flow between the nodes. So in the root node we can see that 337 people died and 233 survived, hence it is blue - as the majority did not survive.

```{r}
# rules it has used to make the tree
rpart.rules(titanic_fit, cover = TRUE)
```

## 1.6 Question 6

Test and add your predictions to your data. Create a confusion matrix. Write down in detail what this tells you for this specific dataset.

```{r}
# add the predictions
titanic_test_pred <- titanic_test %>%
  add_predictions(titanic_fit, type = 'class')
```

Now we can look at our predictions. For the sake of keeping the variables reduced, let’s choose the ones that our decision tree model showed as most informative.

```{r}
# look at the variables 
titanic_test_pred %>%
  select(sex, class, port_embarkation, survived_flag, pred)
```
```{r}
# Check model perfonmance with a confusion matrix
conf_mat <- titanic_test_pred %>%
              conf_mat(truth = survived_flag, estimate = pred)

conf_mat
```

The main diagonal represents correctly-predicted values, with the top right values showing false positives and the bottom left being false negatives. The more accurate the decision tree, the higher the main diagonal values will be. We can calculate that accuracy with a fairly simple calculation (summing the main diagional and diving by the total). The result represents the probability of our prediction being correct. We can use the function accuracy() from yardstick to calculate this.

```{r}
accuracy <- titanic_test_pred %>%
 accuracy(truth = survived_flag, estimate = pred)

accuracy 
```
The .estimate column in the output shows you the probability you have of correctly predicting whether a passenger in the test set survived or not on the Titanic. We can also calculate the sensitivity (AKA true positive rate) and specificity (AKA true negative rate) using other yardstick functions. 

```{r}
titanic_test_pred %>%
  sensitivity(truth = survived_flag, estimate = pred)
```


```{r}
titanic_test_pred %>%
  specificity(truth = survived_flag, estimate = pred)
```

```{r}
confusionMatrix(titanic_test_pred$pred, titanic_test_pred$survived_flag) 
#order is estimate and then truth 
```




# 2 Extension
See how a ranger() random forest classifier compares with a single decision tree in terms of performance. Can you tune the values of the mtry, splitrule and min.node.size hyperparameters? Which variables in the dataset turn out to be most important for your best model? The Kappa metric might be the best one to focus on if you want to improve performance for an imbalanced data set. Do some research on the definition of Kappa before you start.

We provide the code in the dropdown below if you get stuck, but still want to play around with this (note that run time can be up to 5-10 mins for the tuning). Save your notebook before you begin in case you need to force quit your session!


```{r}
control <- trainControl(
  method = "repeatedcv", 
  number = 5, 
  repeats = 10
)

tune_grid = expand.grid(
  mtry = 1:6,
  splitrule = c("gini", "extratrees"),
  min.node.size = c(1, 3, 5)
)
```

```{r}
rf_tune <- train(
  survived_flag ~ ., 
  data = titanic_train, 
  method = "ranger",
  metric = "Kappa",
  num.trees = 1000,
  importance = "impurity",
  tuneGrid = tune_grid, 
  trControl = control
)

plot(rf_tune)
rf_tune
```

